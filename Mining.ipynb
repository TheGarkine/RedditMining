{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit-Mining\n",
    "## Die Ziele im Projekt\n",
    "\n",
    "Allgemein ist die Aufgabe Texte systematisch aus dem Web zu gewinnen und diese dann auszuwerten.\n",
    "\n",
    "Wir haben uns zur Aufgabe gemacht durch diverse Reddit-Spiele-Foren (https://reddit.com) zu crawlen und verschiedene Aussagen über die Nutzer und die Spiele der einzelnen Foren zu machen:\n",
    "    - Wie freundlich sind die Spieler der einzelnen Foren?\n",
    "    - In welchen Spielen wird am meisten im Web diskutiert?\n",
    "    - Welche Spieler sind die Freundlichsten, welche Kriterien nehmen den meisten Einfluss?\n",
    "    \n",
    "Zusätzlich ist es das Ziel, einen beliebigen Text zu einem der Spiele zuzuordnen.\n",
    "\n",
    "Zur Datenspeicherung wird nach Aufgabe eine SQL-Lite Datenbank verwendet.   \n",
    "\n",
    "Wichtig für uns ist Modularität, sodass wir schnell weitere Reddit-Foren hinzufügen können und nach anderen Kriterien suchen können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "Dieser Abschnitt behandelt das Scraping der einzelnen Reddit Pages (Subreddits).\n",
    "\n",
    "Der Spider benutzt folgende Foren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_urls = [\n",
    "    \"https://www.reddit.com/r/DotA2/\",\n",
    "    'https://www.reddit.com/r/GlobalOffensive/',\n",
    "    'https://www.reddit.com/r/leagueoflegends/',\n",
    "    'https://www.reddit.com/r/darksouls3/',\n",
    "    'https://www.reddit.com/r/Witcher3/',\n",
    "    'https://www.reddit.com/r/Smite/',\n",
    "    'https://www.reddit.com/r/aoe4/',\n",
    "    'https://www.reddit.com/r/unrealtournament/',\n",
    "    'https://www.reddit.com/r/battlefield_one/',\n",
    "    'https://www.reddit.com/r/FIFA/',\n",
    "    'https://www.reddit.com/r/WorldofTanks/',\n",
    "    'https://www.reddit.com/r/FortNiteBR/',\n",
    "    'https://www.reddit.com/r/PUBATTLEGROUNDS/',\n",
    "    'https://www.reddit.com/r/starcraft/',\n",
    "    'https://www.reddit.com/r/RocketLeague/',\n",
    "    'https://www.reddit.com/r/CallOfDuty/',\n",
    "    'https://www.reddit.com/r/tf2/',\n",
    "    'https://www.reddit.com/r/skyrim/',\n",
    "    'https://www.reddit.com/r/wow/',\n",
    "    'https://www.reddit.com/r/Guildwars2/',\n",
    "    'https://www.reddit.com/r/silenthill/',\n",
    "    'https://www.reddit.com/r/civ/',\n",
    "    'https://www.reddit.com/r/StreetFighter/',\n",
    "    'https://www.reddit.com/r/smashbros/',\n",
    "    'https://www.reddit.com/r/Mario/',\n",
    "    'https://www.reddit.com/r/Breath_of_the_Wild/',\n",
    "    'https://www.reddit.com/r/farcry/',\n",
    "    'https://www.reddit.com/r/pokemon/',\n",
    "    'https://www.reddit.com/r/Tetris/',\n",
    "    'https://www.reddit.com/r/heroesofthestorm/',\n",
    "    'https://www.reddit.com/r/Tekken/',\n",
    "    'https://www.reddit.com/r/assassinscreed/',\n",
    "    'https://www.reddit.com/r/mariokart/',\n",
    "    'https://www.reddit.com/r/granturismo/',\n",
    "    'https://www.reddit.com/r/forza/',\n",
    "    'https://www.reddit.com/r/HeroesofNewerth/',\n",
    "    'https://www.reddit.com/r/Minecraft/',\n",
    "    'https://www.reddit.com/r/hearthstone/',\n",
    "    'https://www.reddit.com/r/Terraria/',\n",
    "    'https://www.reddit.com/r/halo/',\n",
    "    'https://www.reddit.com/r/GodofWar/',\n",
    "    'https://www.reddit.com/r/Kirby/',\n",
    "    'https://www.reddit.com/r/gtaonline/',\n",
    "    'https://www.reddit.com/r/Wolfenstein/'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die folgenden Beispiele wird die scrapy basierte Funktion aus Blatt 5 verwendet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "#copy of sheet 05\n",
    "def gen_scrapy_response(url):\n",
    "    # define user agent to simulate interactive user\n",
    "    user_agent = 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "    req = requests.get(url, headers={ \"user-agent\": user_agent })\n",
    "    return scrapy.http.TextResponse(req.url, body=req.text, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reddit selbst zählt offenbar nur auf dem Frontend seine eigenen Forenbeiträge (Threads) durch. dies geschieht über einen HTTP-GET Paramemter \"count\".\n",
    "\n",
    "Diesen kann man somit benutzen um die Tiefe des Spiders in abhängigkeit zur Aktuellen gescrapten Website zu bestimmen. Per default werden immer 25 Threads pro Page angegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "response = gen_scrapy_response(start_urls[0])\n",
    "\n",
    "#sets the depth of the spider, has to be a multiple of 25, if it is not, the next multiple of 25 is used\n",
    "max_reddit_count = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Analyse ist uns aufgefallen, dass alle Reddit-Foren zwar unterschiedliche Designs haben, aber immer die selbe Struktur (in Bezug auf den HTML-Baum und die CSS-Klassen) besitzen. Aus diesem Grund können wir das selbe vorgehen auf alle Foren anwenden.\n",
    "\n",
    "Es gibt drei parse-funktionen:\n",
    "    - parse: Die Hauptseiten nach Threads parsen, und den \"next\"-Button finden, um theorethisch durch das gesamte subreddit \n",
    "        zu crawlen (reddit zeigt benutzern aber nur die letzten 1.000 Threads eines Subreddits, alle vorherigen sind \n",
    "        anscheinend archiviert und auf normalem weg unzugänglich)\n",
    "    - parse_comments: Geht durch einene einzelnen Thread und erfasst alle Kommentare mit:\n",
    "        - Erfasser\n",
    "        - Comment-Karma (abhängig von der Resonanz der User zu diesem Kommentar)\n",
    "        - Inhalt\n",
    "        - Thread\n",
    "        - Spiel\n",
    "    - parse_user: Die \"Homepage\" eines einzelnen user parsen um Informationen über:\n",
    "        - Name\n",
    "        - Reddit-Geburtstag (Registrierungsdatum)\n",
    "        - Karma (Die Summe über das gesamte Karma aller Kommentare/Posts)\n",
    "\n",
    "Im Folgenden wird die grobe Struktur dieser drei Funktionen gezeigt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find count value to termiante if needed\n",
    "find_count = re.findall(\"\\?count=([0-9]+)\",response.url)\n",
    "if(len(find_count) > 0):\n",
    "    count = int(find_count[0])\n",
    "    if(count >= self.max_reddit_count):\n",
    "        #now the parse function would return before anything would have been yielded\n",
    "        print(\"return\")\n",
    "\n",
    "print(\"Threads of this Page are:\")\n",
    "for thread in response.css(\"a.comments::attr(href)\").extract():\n",
    "    #yield response.follow(thread, callback=self.parse_comments)\n",
    "    print(thread)\n",
    "\n",
    "next_page = response.css(\"span.next-button > a::attr(href)\").extract_first()\n",
    "\n",
    "print(\"The link to the next page is:\")\n",
    "if next_page is not None:\n",
    "    #yield response.follow(next_page, callback=self.parse,headers={ \"user-agent\": self.user_agent })\n",
    "    print(next_page)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse_comments\n",
    "Zu beachten ist, das höchstwahrscheinlich in der Ausgabe alle \"Scores\" gleich 0 sind, dies kann an zwei Punkten liegen:\n",
    "    - Die Kommentare sind sehr neu, und es hat noch niemand den Kommentar bewertet\n",
    "    - Manche Subredits erlauben das Bewerten erst 180 Minuten nach Erstellung\n",
    "    \n",
    "Um den Output zu minimieren werden nur die ersten 5 Kommentare betrachtet.\n",
    "\n",
    "Ein weiterer Punkt ist, dass oftmals Unicode-Characters enthalten sind, diese werden einfach entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = gen_scrapy_response(response.css(\"a.comments::attr(href)\").extract_first())\n",
    "\n",
    "title = response.css(\"a.title::text\").extract_first()\n",
    "game = re.findall(\"https://www.reddit.com/r/([a-zA-Z0-9_-]+)/\",response.url)[0]\n",
    "for comment in response.css(\"div.comment\")[:5]:\n",
    "    user = comment.css(\"div.entry > p.tagline > a.author::text\").extract_first()\n",
    "    user_link = comment.css(\"div.entry > p.tagline > a.author::attr(href)\").extract_first()\n",
    "    #yield response.follow(userLink, callback=self.parse_user,headers={ \"user-agent\": self.user_agent })\n",
    "    print(user_link)\n",
    "\n",
    "    points = comment.css(\"span.score::attr(title)\").extract_first()\n",
    "    if(not points):\n",
    "        points = \"0\"\n",
    "    points = int(points)\n",
    "    full_comment = comment.css(\"div.usertext-body > div.md\")[0]\n",
    "    text = \" \".join(full_comment.css(\"p::text\").extract())\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    text = re.sub(r'\\\\u[a-zA-Z0-9]{4}','',text)\n",
    "    #yield dict(game=game,score=points,thread=title,user=user,content=text,table_type='comments')\n",
    "    print(dict(game=game,score=points,thread=title,user=user,content=text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse_user\n",
    "\n",
    "User-Pages sind nicht so konsistent gestaltet. Es gibt zwei verscheidene Designs und Strukturen der Seiten:<br>\n",
    "Neu: [dluminous](https://www.reddit.com/user/dluminous)<br>\n",
    "Alt: [babuks2006](https://www.reddit.com/user/babuks2006)<br>\n",
    "Außerdem gibt es gesperrte user (wird von scrapy gefangen, da Reddit einen HTTP-403 Statuscode sendet) und \"+18\" accounts ([catsalways](https://www.reddit.com/over18?dest=/user/catsalways)) welche einfach ignoriert werden.\n",
    "\n",
    "Das nicht alle Benutzer erfasst werden muss in der Analyse berücksichtigt werden.\n",
    "\n",
    "Zudem soll das \"Reddit-alter\" gespeichert werden, dafür wird ein String aus dem Dokument mithilfe von _datetime_ verarbeitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "response = gen_scrapy_response(\"https://www.reddit.com/user/dluminous\")\n",
    "#response = gen_scrapy_response(\"https://www.reddit.com/user/babuks2006\")\n",
    "\n",
    "findname = response.css(\"a.ProfileSidebar__nameTitleLink::text\").extract_first()\n",
    "if(findname):\n",
    "    name = re.findall(\"u/(.*)\",findname)[0]\n",
    "    birthday = response.css(\"div.UserSpecialsListSidebar > div > ul > li > span.UserSpecialsListSidebar__date::text\").extract_first()\n",
    "    birthday = str(datetime.strptime(birthday, '%B %d, %Y'))\n",
    "    karma = int(re.sub(r\"[., ]|Karma\",\"\",str(response.css(\"div.ProfileSidebar__counters > span >span::text\").extract_first())))\n",
    "    #yield dict(name=name,birthday=birthday,karma = karma,table_type='users')\n",
    "    print(dict(name=name,birthday=birthday,karma = karma))\n",
    "else:\n",
    "    name = response.css(\"div.titlebox > h1::text\").extract_first()\n",
    "    if( not name):\n",
    "        #return #ignore this user, this can occur due to +18 rating of the user\n",
    "        print(\"return\")\n",
    "    birthday = response.css(\"span.age > time::attr(datetime)\").extract_first()\n",
    "    birthday = str(datetime.strptime(birthday, '%Y-%m-%dT%H:%M:%S+00:00'))\n",
    "    karma_post = int(re.sub(r\",\",\"\",response.css(\"span.karma::text\").extract_first()))\n",
    "    karma_comment = int(re.sub(r\",\",\"\",response.css(\"span.comment-karma::text\").extract_first()))\n",
    "    #yield dict(name=name,birthday=birthday,karma = karma_post + karma_comment,table_type='users')\n",
    "    print(dict(name=name,birthday=birthday,karma = karma_post + karma_comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Alle Daten wurden über den Spider in .jl Files gespeichert, diese sind für den Nächsten abstand erforderlich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenbank\n",
    "\n",
    "Für die weitere Analyse werden die Daten in einer SQL-Lite Datenbank gespeichert.\n",
    "\n",
    "Hierfür werden zunächst die Dateien _users.jl_ und _comments.jl_ ausgelesen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "users_file = 'resources/users.jl'\n",
    "\n",
    "users = []\n",
    "with open(users_file) as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line)\n",
    "        users.append(item)\n",
    "        \n",
    "comments_file = 'resources/comments.jl'\n",
    "\n",
    "comments = []\n",
    "with open(comments_file) as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line)\n",
    "        comments.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun sollten die Daten in python benutzbar sein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comments[0])\n",
    "print(len(comments))\n",
    "print(users[0])\n",
    "print(len(users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können die Daten in eine Sqlite Datenbank übernommen werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "database=\"resources/reddit.sqlite\"\n",
    "#Hilfsfunktion von Blatt 06\n",
    "def sql_execute(database, sql):\n",
    "    with sqlite3.connect(database) as con:\n",
    "        cur = con.cursor()\n",
    "        # damit man sieht, was passiert\n",
    "        print(\"Executing in %s:\\n%s\" % (database, sql))\n",
    "        cur.execute(sql)\n",
    "        if cur.rowcount >= 0:\n",
    "            print(cur.rowcount, \" rows affected.\")\n",
    "        print(\"--> OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clearing Database if needed:\n",
    "if(1):\n",
    "    sql_execute(database,\"DROP TABLE users\");\n",
    "    sql_execute(database,\"DROP TABLE comments\");\n",
    "    \n",
    "\n",
    "#creating Tables:\n",
    "sql_execute(database,\"CREATE TABLE IF NOT EXISTS users (name TEXT,birthday TEXT,karma INTEGER)\");\n",
    "sql_execute(database,\"CREATE TABLE IF NOT EXISTS comments (game TEXT,score Integer,thread TEXT,content TEXT,user TEXT)\");\n",
    "\n",
    "#copying data reused function from sheet 06:\n",
    "with sqlite3.connect(database) as con:\n",
    "    cur = con.cursor()\n",
    "\n",
    "    cnt = 0\n",
    "    for user in users:\n",
    "\n",
    "        sql = \"INSERT INTO users (name,birthday,karma) VALUES ('%s','%s',%i)\" % (user['name'],user['birthday'],user['karma'])\n",
    "        try:\n",
    "            cur.execute(sql)\n",
    "            cnt += 1\n",
    "        except Exception as e:\n",
    "            print(\"EXCEPTION: %s \" % e)\n",
    "            print(sql)\n",
    "            print(user)\n",
    "            raise\n",
    "    \n",
    "    print(\"%d records inserted into %s.\" % (cnt, \"users\"))\n",
    "    cnt = 0\n",
    "    for comment in comments:\n",
    "\n",
    "        if(len(comment['content']) > 0):\n",
    "            sql = \"INSERT INTO comments (game,thread,content,score,user) VALUES ('%s','%s','%s',%i,'%s')\" %\\\n",
    "            (comment['game'],re.sub(\"'\",\"''\",comment['thread']),re.sub(\"'\",\"''\",comment['content']),comment['score'],comment['user'])\n",
    "            try:\n",
    "                cur.execute(sql)\n",
    "                cnt += 1\n",
    "            except Exception as e:\n",
    "                print(\"EXCEPTION: %s \" % e)\n",
    "                print(sql)\n",
    "                print(user)\n",
    "                raise\n",
    "\n",
    "    print(\"%d records inserted into %s.\" % (cnt, \"comments\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "Nachdem alle Daten in die Datenkbank geschrieben wurden kann nun versucht werden, ein paar erste Erfahrungen mit den Daten zu machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%sql sqlite:///resources/reddit.sqlite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beispielsweise könnte man versuchen die beliebtesten Kommentare zu finden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM comments ORDER BY score DESC LIMIT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order die unbeliebtesten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM comments ORDER BY score ASC LIMIT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An diesem Punkt könnte man bereits vergleichen wie hoch der durchschnittliche Karma-Score eines Kommentar eines Forum ist.\n",
    "\n",
    "Zur veranschaulichung wird _matplotlib_ verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores = %sql SELECT game,AVG(score) as avg_score FROM comments GROUP BY game ORDER BY avg_score DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#Function from sheet06, modified\n",
    "def txa_plot(list_of_tuples, columns=('Iterator', 'Wert'), title=None,dx = .5,dy=.25): \n",
    "\n",
    "    labels = [t[0] for t in list_of_tuples]\n",
    "    values = [t[1] for t in list_of_tuples]\n",
    "    dummy_pos = np.arange(len(labels))\n",
    "    \n",
    "    # Erstellung des Balkendiagramms\n",
    "    fig=plt.figure(figsize=(18, 10))\n",
    "    plt.bar(dummy_pos, values)\n",
    "    plt.xticks(dummy_pos, labels,  rotation=\"vertical\", fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.xlabel(columns[0], fontsize=20)\n",
    "    plt.ylabel(columns[1], fontsize=20)\n",
    "    #displays the value of a bar above the bar\n",
    "    for i,v in enumerate(values):\n",
    "        plt.text(i - dx,v + dy,str(\"%.1f\" % v),color=\"black\",fontsize=10)\n",
    "    if title: \n",
    "        plt.title(title, fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txa_plot(average_scores,('Spiel','Mittelwert'),'Mittelwerte der Karma-scores der einzelnen Kommentare nach Forum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auffällig ist, das alte Spiele, und Spiele mit kleinen Fangemeinden schlechter Abschneiden. Man könnte versuchen die Relevanz zu erhöhen in dem man dies mit der Aktivität in den Foren gewichtet.\n",
    "\n",
    "Hierfür wäre es interessant zu wissen, wie viele Kommentare im Schnitt pro Thread erfasst wurden, zu beachten ist hierbei, dass zu tiefe Verschachtelungen von Kommentaren und antworten nicht mitgescrapt sind, da diese nur über Javascript aufruf bar sind.\n",
    "\n",
    "Zudem sollte man erwähnen, dass nicht nur kommentierende User existieren, es wird also die Annahme getroffen, dass die Kommentaranzahl linear propertional zur Gesamtaktivität ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_comments = %sql SELECT game,AVG(count) as avg FROM (SELECT COUNT(*) as count,game FROM comments GROUP BY thread) GROUP BY game ORDER BY avg DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "txa_plot(average_comments,('Spiel','Mittelwert'),\"Mittelwerte der Aktivität pro Thread pro Spiel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_score_by_activity = %sql SELECT avgs.game,avg_score/avg_count as avg,avg_score,avg_count from (SELECT game,AVG(score) as avg_score FROM comments GROUP BY game) as avgs JOIN (SELECT game,AVG(count) as avg_count FROM (SELECT COUNT(*) as count,game FROM comments GROUP BY thread) GROUP BY game) as avgc  ON avgc.game = avgs.game ORDER BY avg DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txa_plot(average_score_by_activity,('Spiel','Mittelwert'),\"Mittelwert der Scores/Mittelwert der Kommentaranzahl\",dx=.33,dy=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "average_score_by_activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als letztes wäre noch interessant zu wissen, wie lang die einzelnen Kommentare im Schnitt sind. Eventuell haben länge Kommentare im Schnitt einen höheren Score. Es wird versucht dies herauszufinden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = %sql select content,score from comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_words_score = [(len(re.split(r' ',x[0])),x[1]) for x in comments] # wörter\n",
    "#comments_words_score = [(len(x[0]),x[1]) for x in comments] #Zeichen\n",
    "#comments_words_score = [(len(re.split(r' ',x[0])),x[1]) for x in comments if x[1]<1000 and x[1] > 0] # wörter, etwas geglättet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 10))\n",
    "x = [t[0] for t in comments_words_score]\n",
    "y = [t[1] for t in comments_words_score]\n",
    "plt.plot(x,y,linestyle='none',marker='.',markersize=4)\n",
    "plt.xlabel(\"Länge\", fontsize=20)\n",
    "plt.ylabel(\"Score\", fontsize=20)\n",
    "plt.title(\"Scores nach Kommentarlänge\",fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Maß für den Zusammenhang könnte man Pearson hernehmen (gilt nur bei linearem Zusammenhang)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pearson(list_of_coordinates):\n",
    "    n = len(list_of_coordinates)\n",
    "    #mean values\n",
    "    mx = 0.0\n",
    "    my = 0.0\n",
    "    for el in list_of_coordinates:\n",
    "        mx += el[0]\n",
    "        my += el[1]\n",
    "    mx /= n\n",
    "    my /= n\n",
    "    \n",
    "    #calculate variances and covariance\n",
    "    varx = 0.0\n",
    "    vary = 0.0\n",
    "    cov = 0.0\n",
    "    for el in list_of_coordinates:\n",
    "        cov += (el[0]-mx)*(el[1]-my)\n",
    "        varx += (el[0]-mx)**2\n",
    "        vary += (el[1]-my)**2\n",
    "    cov /=(n-1)\n",
    "    varx /=(n-1)\n",
    "    vary /=(n-1)\n",
    "    sx = varx**(1/2)\n",
    "    sy = vary**(1/2)\n",
    "    pearson = cov/(sx*sy)\n",
    "    return pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calc_pearson(comments_words_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offenbar besteht jedoch kein (linearer) Zusammenhang zwischen Länge eines Kommentar und dessen Score. Grund dafür könnten die kurzen Kommentare mit extrem hohen Scores und die Langen kommentare mit extrem niedrigen sein. Auf einem Fachforum wie http://stackoverflow.com wäre dieses Ergebnis eventuell anders.\n",
    "\n",
    "Noch eine Interessante Analyse wäre das Vorkommen von Smilies und emoticons in den Texten. Es werden nur Smilies aus normalen ASCII-Zeichen betrachtet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from collections import Counter\n",
    "#extracts list of (\"smilie\",counter) for given text\n",
    "def count_normalized_smilies (text):\n",
    "    smilies = re.findall(r\"[:=]-?P|[:=]-?\\(|[:=]-?\\)|[:=]-?D|[:=]-?O|[:=]-?\\*|;-?\\)|[oO]\\.[oO]|\\*\\.\\*\",text) #content column\n",
    "    normalized_smilies = []\n",
    "    for smilie in smilies:\n",
    "        normalized_smilies.append(re.sub(\"=\",\":\",re.sub(\"-\",\"\",smilie)).upper())\n",
    "    return Counter(normalized_smilies).most_common()\n",
    "\n",
    "with sqlite3.connect(database) as con:\n",
    "    smilie_df = pandas.read_sql_query(\"SELECT game, group_concat(content,' ') as text FROM comments GROUP BY game ORDER BY game\",con)\n",
    "\n",
    "smilie_df[\"smilie_count\"] = smilie_df.text.map(count_normalized_smilies)\n",
    "\n",
    "# format dataframe for plot function:\n",
    "smilies_plotable = []\n",
    "\n",
    "#agg_df.head(10)\n",
    "for index, row in smilie_df.iterrows():\n",
    "    game = row['game']\n",
    "    c = row['smilie_count']\n",
    "    for k in c:\n",
    "        smilies_plotable.append((game,k[0],k[1]))           \n",
    "#your code here\n",
    "\n",
    "smilies_plotable_df = pandas.DataFrame.from_records(smilies_plotable, columns=['Spiel', 'Smilie', 'Anzahl'])\n",
    "smilies_plotable_df = smilies_plotable_df.pivot(index=smilies_plotable_df.columns[0], columns=smilies_plotable_df.columns[1], values=smilies_plotable_df.columns[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smilies_plotable_df.plot(kind=\"bar\",stacked=True,figsize=(17,12),title=\"Häufigkeit ausgewählter Smilies in den Einzelnen Foren\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuletzt könnte man noch erkennen wie sich die Kommentare über die Spiel verteilen. Um das ganze einfach zu plotten wird hierfür schon ein Dataframe bentuzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(database) as con:\n",
    "    comments_per_category = pandas.read_sql_query(\"SELECT COUNT(*) as count, game FROM comments GROUP BY game ORDER BY count DESC\", con)\n",
    "comments_per_category.plot.pie(labels=comments_per_category.game,y=\"count\",figsize=(15,12),legend=False,title=\"Verteilung der Kommentare in der Datenbank\",title.font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klassifizierung\n",
    "\n",
    "Für die Klassifizierung müssen zunächst die Daten aufbereitet werden. Hierfür werden zunächst alle Kommentare in ein dataframe gezogen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(database) as con:\n",
    "    comments_df = pandas.read_sql_query(\"SELECT * FROM comments\", con)\n",
    "comments_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann wird eine eigene Pipeline definiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "punctuation = set(string.punctuation) | {'``', \"''\", '–',\"'\",\"’\",'\\\\','/','(',')',';','%','?','!','_',':'}\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    return [t for t in tokens if t not in punctuation]\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "custom_stopwords = {\"r\",\"u\",\"im\",\"get\", '\\'s', '\\'m'}\n",
    "stopwords = stopwords.union(custom_stopwords)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t.lower() not in stopwords]\n",
    "\n",
    "def remove_numbers(tokens):\n",
    "    return [t for t in tokens if not re.findall(r'[0-9]+',t)]\n",
    "\n",
    "def preprocess(text):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_numbers(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "processed_comments_df = comments_df[['game', 'score','thread','user','content']].copy()\n",
    "\n",
    "processed_comments_df[\"processed_content\"] = comments_df[\"content\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_words_list = []\n",
    "for comment_words in processed_comments_df.processed_content:\n",
    "    all_words_list += comment_words\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "Für die Sentiment Analyse steht durch das NLTK bereits VADER zur verfügung, welches bereits in der Lage ist über sein Wörterbuch eine regelbasierte Einstufung vorzunehmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst wird eine Datenserie erzeugt, die sich aus der Abbildung der Inhaltsspalte durch die polarity_scores-Methode des SentimentIntensityAnalyzers ergibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "processed_with_sent = processed_comments_df.copy()\n",
    "sentiment_series = processed_with_sent[\"content\"].map(lambda text: intensity_analyzer.polarity_scores(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Inhalt dieser Datenserie hat mit einem Dictionary pro Zeile noch ein recht unvorteilhaftes Format,\n",
    "wenn wir später komfortabel und schnell auf die Werte zugreifen wollen.\n",
    "Aufgrund dessen wird jeder Wert des Dictionaries nun einer Spalte des Dataframes zugeordnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_with_sent[\"sent_neg\"] = sentiment_series.map(lambda x: x[\"neg\"])\n",
    "processed_with_sent[\"sent_neu\"] = sentiment_series.map(lambda x: x[\"neu\"])\n",
    "processed_with_sent[\"sent_pos\"] = sentiment_series.map(lambda x: x[\"pos\"])\n",
    "processed_with_sent[\"sent_comp\"] = sentiment_series.map(lambda x: x[\"compound\"])\n",
    "\n",
    "processed_with_sent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun sind die Voraussetzungen geschaffen, um effizient auf den Daten arbeiten zu können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Durchschnittliche Scores\n",
    "\n",
    "Eine simple Auswertung ist beispielsweise das Bilden der durchschnittlichen Bewertungen der Sentiment-Analyse pro Spiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_mean = {}\n",
    "sent_mean[\"neg\"] = processed_with_sent.groupby(['game'])[\"sent_neg\"].mean().sort_values(ascending=False)\n",
    "sent_mean[\"neu\"] = processed_with_sent.groupby(['game'])[\"sent_neu\"].mean().sort_values(ascending=False)\n",
    "sent_mean[\"pos\"] = processed_with_sent.groupby(['game'])[\"sent_pos\"].mean().sort_values(ascending=False)\n",
    "sent_mean[\"comp\"] = processed_with_sent.groupby(['game'])[\"sent_comp\"].mean().sort_values(ascending=False)\n",
    "sent_mean[\"steadiness\"] = (sent_mean[\"neg\"] + sent_mean[\"pos\"]).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negativstes Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_mean[\"neg\"].plot(kind=\"bar\", figsize=(15,12), title=\"Negativstes Subreddit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positivstes Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_mean[\"pos\"].plot(kind=\"bar\", figsize=(15,12), title=\"Positivstes Subreddit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subrredit mit der besten durchschnittlichen Gesamtwertung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_mean[\"comp\"].plot(kind=\"bar\", figsize=(15,12), title=\"Durchschnittlich bestes Subreddit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subreddit mit dem größten Abstand zwischen positivem und negativem Durschnitt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_mean[\"steadiness\"].plot(kind=\"bar\", figsize=(15,12), title=\"schizophrenstes Subreddit\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
