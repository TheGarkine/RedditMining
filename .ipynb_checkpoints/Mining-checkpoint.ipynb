{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit-Mining\n",
    "## Die Ziele im Projekt\n",
    "\n",
    "Allgemein ist die Aufgabe Texte systematisch aus dem Web zu gewinnen und diese dann auszuwerten.\n",
    "\n",
    "Wir haben uns zur Aufgabe gemacht durch diverse Reddit-Spiele-Foren (https://reddit.com) zu crawlen und verschiedene Aussagen über die Nutzer und die Spiele der einzelnen Foren zu machen:\n",
    "    - Wie freundlich sind die Spieler der einzelnen Foren?\n",
    "    - In welchen Spielen wird am meisten im Web diskutiert?\n",
    "    - Welche Spieler sind die Freundlichsten, welche Kriterien nehmen den meisten Einfluss?\n",
    "    \n",
    "Zusätzlich ist es das Ziel, einen beliebigen Text zu einem der Spiele zuzuordnen.\n",
    "\n",
    "Zur Datenspeicherung wird nach Aufgabe eine SQL-Lite Datenbank verwendet.   \n",
    "\n",
    "Wichtig für uns ist Modularität, sodass wir schnell weitere Reddit-Foren hinzufügen können und nach anderen Kriterien suchen können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "Dieser Abschnitt behandelt das Scraping der einzelnen Reddit Pages (Subreddits).\n",
    "\n",
    "Der Spider benutzt folgende Foren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_urls = [\n",
    "\t\t\"https://www.reddit.com/r/DotA2/\",\n",
    "\t\t'https://www.reddit.com/r/GlobalOffensive/',\n",
    "\t\t#'https://www.reddit.com/r/spacex/', <- War lediglich aus interesse\n",
    "\t\t'https://www.reddit.com/r/leagueoflegends/',\n",
    "\t\t'https://www.reddit.com/r/darksouls3/',\n",
    "\t\t'https://www.reddit.com/r/Witcher3/',\n",
    "\t\t'https://www.reddit.com/r/Smite/',\n",
    "\t\t'https://www.reddit.com/r/aoe4/',\n",
    "\t\t'https://www.reddit.com/r/unrealtournament/',\n",
    "\t\t'https://www.reddit.com/r/battlefield_one/',\n",
    "\t\t'https://www.reddit.com/r/FIFA/',\n",
    "\t\t'https://www.reddit.com/r/WorldofTanks/',\n",
    "\t\t'https://www.reddit.com/r/FortNiteBR/',\n",
    "\t\t'https://www.reddit.com/r/PUBATTLEGROUNDS/',\n",
    "\t\t'https://www.reddit.com/r/starcraft/',\n",
    "\t\t'https://www.reddit.com/r/RocketLeague/',\n",
    "\t\t'https://www.reddit.com/r/CallOfDuty/',\n",
    "\t\t'https://www.reddit.com/r/tf2/',\n",
    "\t\t'https://www.reddit.com/r/skyrim/',\n",
    "\t\t'https://www.reddit.com/r/wow/',\n",
    "\t\t'https://www.reddit.com/r/Guildwars2/',\n",
    "\t\t'https://www.reddit.com/r/silenthill/',\n",
    "\t\t'https://www.reddit.com/r/civ/',\n",
    "\t\t'https://www.reddit.com/r/StreetFighter/',\n",
    "\t\t'https://www.reddit.com/r/smashbros/',\n",
    "\t\t'https://www.reddit.com/r/Mario/',\n",
    "\t\t'https://www.reddit.com/r/Breath_of_the_Wild/',\n",
    "\t\t'https://www.reddit.com/r/farcry/',\n",
    "\t\t'https://www.reddit.com/r/pokemon/',\n",
    "\t\t'https://www.reddit.com/r/Tetris/',\n",
    "\t\t'https://www.reddit.com/r/heroesofthestorm/',\n",
    "\t\t'https://www.reddit.com/r/Tekken/',\n",
    "\t\t'https://www.reddit.com/r/assassinscreed/',\n",
    "\t\t'https://www.reddit.com/r/mariokart/',\n",
    "\t\t'https://www.reddit.com/r/granturismo/',\n",
    "\t\t'https://www.reddit.com/r/forza/',\n",
    "\t\t'https://www.reddit.com/r/HeroesofNewerth/',\n",
    "\t\t'https://www.reddit.com/r/Minecraft/',\n",
    "\t\t'https://www.reddit.com/r/hearthstone/',\n",
    "\t\t'https://www.reddit.com/r/Terraria/',\n",
    "\t\t'https://www.reddit.com/r/halo/',\n",
    "\t\t'https://www.reddit.com/r/GodofWar/',\n",
    "\t\t'https://www.reddit.com/r/Kirby/',\n",
    "\t\t'https://www.reddit.com/r/gtaonline/',\n",
    "\t\t'https://www.reddit.com/r/Wolfenstein/'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die folgenden Beispiele wird die scrapy basierte Funktion aus Blatt 5 verwendet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "#copy of sheet 05\n",
    "def gen_scrapy_response(url):\n",
    "    # define user agent to simulate interactive user\n",
    "    user_agent = 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "    req = requests.get(url, headers={ \"user-agent\": user_agent })\n",
    "    return scrapy.http.TextResponse(req.url, body=req.text, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reddit selbst zählt offenbar nur auf dem Frontend seine eigenen Forenbeiträge (Threads) durch. dies geschieht über einen HTTP-GET Paramemter \"count\".\n",
    "\n",
    "Diesen kann man somit benutzen um die Tiefe des Spiders in abhängigkeit zur Aktuellen gescrapten Website zu bestimmen. Per default werden immer 25 Threads pro Page angegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "response = gen_scrapy_response(start_urls[0])\n",
    "\n",
    "#sets the depth of the spider, has to be a multiple of 25, if it is not, the next multiple of 25 is used\n",
    "max_reddit_count = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Analyse ist uns aufgefallen, dass alle Reddit-Foren zwar unterschiedliche Designs haben, aber immer die selbe Struktur (auch in Bezug auf den HTML-Baum und die CSS-Klassen) besitzen. Aus diesem Grund können wir das selbe vorgehen auf alle Foren anwenden.\n",
    "\n",
    "Es gibt drei parse-funktionen:\n",
    "    - parse: Die Hauptseiten nach Threads parsen, und den \"next\"-Button finden, um theorethisch durch das gesamte subreddit \n",
    "        zu crawlen (reddit zeigt benutzern aber nur die letzten 1.000 Threads eines Subreddits, alle vorherigen sind \n",
    "        anscheinend archiviert und auf normalem weg unzugänglich)\n",
    "    - parse_comments: Geht durch einene einzelnen Thread und erfasst alle Kommentare mit:\n",
    "        - Erfasser\n",
    "        - Comment-Karma (abhängig von der Resonanz der User zu diesem Kommentar)\n",
    "        - Inhalt\n",
    "        - Thread\n",
    "        - Spiel\n",
    "    - parse_user: Die \"Homepage\" eines einzelnen user parsen um Informationen über:\n",
    "        - Name\n",
    "        - Reddit-Geburtstag (Registrierungsdatum)\n",
    "        - Karma (Die Summe über das gesamte Karma aller Kommentare/Posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Struktur von Parse ist somit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threads of this Page are:\n",
      "https://www.reddit.com/r/DotA2/comments/8gwz56/epicenter_xl_day_7_match_discussions/\n",
      "https://www.reddit.com/r/DotA2/comments/8gw62o/esl_shutting_down_csgo_streamers_so_they_go_to/\n",
      "https://www.reddit.com/r/DotA2/comments/8gudcj/naix_finally_turned_into_nyx/\n",
      "https://www.reddit.com/r/DotA2/comments/8gu0wo/2_new_unnamed_dlcs_were_added_to_dota_2_just_a/\n",
      "https://www.reddit.com/r/DotA2/comments/8gra6p/introducing_dota_2_battle_royale/\n",
      "https://www.reddit.com/r/DotA2/comments/8gtpp5/ice_fire_and_memes/\n",
      "https://www.reddit.com/r/DotA2/comments/8gtspk/dota_2_update_main_client_may_3_2018/\n",
      "https://www.reddit.com/r/DotA2/comments/8gswvi/valve_pls_let_us_choose_the_voice_line_for_lvl25/\n",
      "https://www.reddit.com/r/DotA2/comments/8gveyb/promocode_bsj_chronosphere/\n",
      "https://www.reddit.com/r/DotA2/comments/8gsdey/with_poor_mans_shield_gone_can_we_get_slippers/\n",
      "https://www.reddit.com/r/DotA2/comments/8gohzm/anyone_here_doesnt_really_play_dota_that_much_but/\n",
      "https://www.reddit.com/r/DotA2/comments/8gv4hz/i_painted_night_stalker/\n",
      "https://www.reddit.com/r/DotA2/comments/8gwuwn/sword_warrior_fanart/\n",
      "https://www.reddit.com/r/DotA2/comments/8grkod/thanks_for_turbo_dota_devs/\n",
      "https://www.reddit.com/r/DotA2/comments/8gqfcn/i_singlehandedly_made_dota_popular_in_my_uni/\n",
      "https://www.reddit.com/r/DotA2/comments/8gulsj/ti8_compendium_coming_soon_its_time_to_auras_volva/\n",
      "https://www.reddit.com/r/DotA2/comments/8gscqe/offlane_pulling_in_patch_714_fight_back_against/\n",
      "https://www.reddit.com/r/DotA2/comments/8gpsay/typical_ancient_game/\n",
      "https://www.reddit.com/r/DotA2/comments/8gvmny/pain_rangers/\n",
      "https://www.reddit.com/r/DotA2/comments/8gptcw/dota_pre580_wasnt_fun/\n",
      "https://www.reddit.com/r/DotA2/comments/8gunq8/im_going_to_go_watch_some_anime/\n",
      "https://www.reddit.com/r/DotA2/comments/8gszkw/an_underground_puck_set_for_ti8/\n",
      "https://www.reddit.com/r/DotA2/comments/8gp9ri/anyone_here_who_sincerely_wants_to_report_anyone/\n",
      "https://www.reddit.com/r/DotA2/comments/8goan6/we_need_this_back/\n",
      "https://www.reddit.com/r/DotA2/comments/8gvth1/it_do_be_like_that_sometimes/\n",
      "https://www.reddit.com/r/DotA2/comments/8gpnx7/remove_the_recipe_for_boots_of_travel_and_replace/\n",
      "The link to the next page is:\n",
      "https://www.reddit.com/r/DotA2/?count=25&after=t3_8gpnx7\n"
     ]
    }
   ],
   "source": [
    "#Find count value to termiante if needed\n",
    "find_count = re.findall(\"\\?count=([0-9]+)\",response.url)\n",
    "if(len(find_count) > 0):\n",
    "    count = int(find_count[0])\n",
    "    if(count >= self.max_reddit_count):\n",
    "        #now the parse function would return before anything would have been yielded\n",
    "        print(\"return\")\n",
    "\n",
    "print(\"Threads of this Page are:\")\n",
    "for thread in response.css(\"a.comments::attr(href)\").extract():\n",
    "    #yield response.follow(thread, callback=self.parse_comments)\n",
    "    print(thread)\n",
    "\n",
    "next_page = response.css(\"span.next-button > a::attr(href)\").extract_first()\n",
    "\n",
    "print(\"The link to the next page is:\")\n",
    "if next_page is not None:\n",
    "    #yield response.follow(next_page, callback=self.parse,headers={ \"user-agent\": self.user_agent })\n",
    "    print(next_page)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse_comments ist so aufgebaut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'yield' outside function (<ipython-input-38-da0db24e455b>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-38-da0db24e455b>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    yield response.follow(userLink, callback=self.parse_user,headers={ \"user-agent\": self.user_agent })\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'yield' outside function\n"
     ]
    }
   ],
   "source": [
    "response = gen_scrapy_response(response.css(\"a.comments::attr(href)\").extract_first())\n",
    "\n",
    "title = response.css(\"a.title::text\").extract_first()\n",
    "game = re.findall(\"https://www.reddit.com/r/([a-zA-Z0-9_-]+)/\",response.url)[0]\n",
    "for comment in response.css(\"div.comment\"):\n",
    "    user = comment.css(\"div.entry > p.tagline > a.author::text\").extract_first()\n",
    "    user_link = comment.css(\"div.entry > p.tagline > a.author::attr(href)\").extract_first()\n",
    "    #yield response.follow(userLink, callback=self.parse_user,headers={ \"user-agent\": self.user_agent })\n",
    "    print(user_link)\n",
    "\n",
    "    points = comment.css(\"span.score::attr(title)\").extract_first()\n",
    "    if(not points):\n",
    "        points = \"0\"\n",
    "    points = int(points)\n",
    "    myComment = comment.css(\"div.usertext-body > div.md\")[0]\n",
    "    text = \" \".join(myComment.css(\"p::text\").extract())\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    yield dict(game=game,\n",
    "    score=points,\n",
    "    thread=title,\n",
    "    user=user,\n",
    "    #url=response.url,\n",
    "    content=text,\n",
    "    table_type='comments'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem alle Links erfasst wurden, kann man durch diese Listen iterieren um die Daten zu erfassen. Die Daten werden in einer Liste aus Dictionaries gespeichert, diese dcits besitzen zunächst folgende Informationen:\n",
    "- game: Das Spiel für welches dieser Kommentar geschrieben wurde\n",
    "- thread: Der Titel des Threads, für welchen diese Kommentar geschrieben wurde\n",
    "- user: Der Benutzer der diesen Kommentar geschrieben hat\n",
    "- content: Der Text-Inhalt des Kommentars\n",
    "\n",
    "Um gleich die Daten etwas aufzubereiten werden Regular Expressions verwendet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "all_comments = []\n",
    "#iterating through the games\n",
    "for game in reddit_threads:\n",
    "    #creating the new dictionary\n",
    "    comment = {}\n",
    "    #iterating through the threads of a game\n",
    "    for thread in reddit_threads[game]:\n",
    "        response = gen_scrapy_response(thread)\n",
    "        title = response.css(\"a.title::text\").extract_first()\n",
    "        comments = response.css(\"div.comment\")\n",
    "        for comment in comments:\n",
    "            user = comment.css(\"div.entry > p.tagline > a.author::text\").extract_first()\n",
    "            text = \" \".join(comment.css(\"div.usertext-body > div.md > *::text\").extract())\n",
    "            text = re.sub('\\s+',' ',text)\n",
    "            all_comments.append({\"game\":game,\"thread\":title,\"user\":user,\"content\":text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4787\n"
     ]
    }
   ],
   "source": [
    "print(len(all_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/DotA2/\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "link = \"https://www.reddit.com/r/DotA2/?count=10&after=t3_8f1w5j\"\n",
    "x = re.findall(r'(https://www.reddit.com/r/\\w+/)',link)[0]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenbank\n",
    "\n",
    "Für die weitere Analyse werden die Daten in einer relationalen SQL-Lite Datenbank gespeichert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-17 22:50:01\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "datetime_object = datetime.strptime('2018-01-17T22:50:01+00:00', '%Y-%m-%dT%H:%M:%S+00:00')\n",
    "print(datetime_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "def bar_plot(list_of_tuples, columns=('Wort', 'Anzahl'), title=None): \n",
    "\n",
    "    labels = [t[0] for t in list_of_tuples]\n",
    "    values = [t[1] for t in list_of_tuples]\n",
    "    dummy_pos = np.arange(len(labels))\n",
    "    \n",
    "    # Erstellung des Balkendiagramms\n",
    "    fig=plt.figure(figsize=(18, 10))\n",
    "    plt.bar(dummy_pos, values)\n",
    "    plt.xticks(dummy_pos, labels,  rotation=\"vertical\", fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.xlabel(columns[0], fontsize=20)\n",
    "    plt.ylabel(columns[1], fontsize=20)\n",
    "    if title: \n",
    "        plt.title(title, fontsize=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
